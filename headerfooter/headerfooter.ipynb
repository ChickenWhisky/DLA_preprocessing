{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "8wmu7nkehi31",
        "outputId": "98debfe5-2072-476c-b241-5f457668342d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e6e7beb9f83e>\u001b[0m in \u001b[0;36m<cell line: 112>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;31m# Otherwise, you need to load a model using multiple GPUs, please refer to the `Multiple GPUs` section.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"OpenGVLab/InternVL2-1B\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m \u001b[0mdevice_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'InternVL2-1B'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m model = AutoModel.from_pretrained(\n\u001b[1;32m    114\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-e6e7beb9f83e>\u001b[0m in \u001b[0;36msplit_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mnum_layers_per_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mnum_layers_per_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum_layers_per_gpu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mnum_layers_per_gpu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers_per_gpu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mlayer_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers_per_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import math\n",
        "\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "def build_transform(input_size):\n",
        "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
        "    transform = T.Compose([\n",
        "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
        "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=MEAN, std=STD)\n",
        "    ])\n",
        "    return transform\n",
        "\n",
        "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
        "    best_ratio_diff = float('inf')\n",
        "    best_ratio = (1, 1)\n",
        "    area = width * height\n",
        "    for ratio in target_ratios:\n",
        "        target_aspect_ratio = ratio[0] / ratio[1]\n",
        "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
        "        if ratio_diff < best_ratio_diff:\n",
        "            best_ratio_diff = ratio_diff\n",
        "            best_ratio = ratio\n",
        "        elif ratio_diff == best_ratio_diff:\n",
        "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
        "                best_ratio = ratio\n",
        "    return best_ratio\n",
        "\n",
        "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
        "    orig_width, orig_height = image.size\n",
        "    aspect_ratio = orig_width / orig_height\n",
        "\n",
        "    # calculate the existing image aspect ratio\n",
        "    target_ratios = set(\n",
        "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
        "        i * j <= max_num and i * j >= min_num)\n",
        "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
        "\n",
        "    # find the closest aspect ratio to the target\n",
        "    target_aspect_ratio = find_closest_aspect_ratio(\n",
        "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
        "\n",
        "    # calculate the target width and height\n",
        "    target_width = image_size * target_aspect_ratio[0]\n",
        "    target_height = image_size * target_aspect_ratio[1]\n",
        "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
        "\n",
        "    # resize the image\n",
        "    resized_img = image.resize((target_width, target_height))\n",
        "    processed_images = []\n",
        "    for i in range(blocks):\n",
        "        box = (\n",
        "            (i % (target_width // image_size)) * image_size,\n",
        "            (i // (target_width // image_size)) * image_size,\n",
        "            ((i % (target_width // image_size)) + 1) * image_size,\n",
        "            ((i // (target_width // image_size)) + 1) * image_size\n",
        "        )\n",
        "        # split the image\n",
        "        split_img = resized_img.crop(box)\n",
        "        processed_images.append(split_img)\n",
        "    assert len(processed_images) == blocks\n",
        "    if use_thumbnail and len(processed_images) != 1:\n",
        "        thumbnail_img = image.resize((image_size, image_size))\n",
        "        processed_images.append(thumbnail_img)\n",
        "    return processed_images\n",
        "\n",
        "def load_image(image_file, input_size=448, max_num=12):\n",
        "    image = Image.open(image_file).convert('RGB')\n",
        "    transform = build_transform(input_size=input_size)\n",
        "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
        "    pixel_values = [transform(image) for image in images]\n",
        "    pixel_values = torch.stack(pixel_values)\n",
        "    return pixel_values\n",
        "\n",
        "def split_model(model_name):\n",
        "    device_map = {}\n",
        "    world_size = torch.cuda.device_count()\n",
        "    num_layers = {\n",
        "        'InternVL2-1B': 24, 'InternVL2-2B': 24, 'InternVL2-4B': 32, 'InternVL2-8B': 32,\n",
        "        'InternVL2-26B': 48, 'InternVL2-40B': 60, 'InternVL2-Llama3-76B': 80}[model_name]\n",
        "    # Since the first GPU will be used for ViT, treat it as half a GPU.\n",
        "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
        "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
        "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
        "    layer_cnt = 0\n",
        "    for i, num_layer in enumerate(num_layers_per_gpu):\n",
        "        for j in range(num_layer):\n",
        "            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
        "            layer_cnt += 1\n",
        "    device_map['vision_model'] = 0\n",
        "    device_map['mlp1'] = 0\n",
        "    device_map['language_model.model.tok_embeddings'] = 0\n",
        "    device_map['language_model.model.embed_tokens'] = 0\n",
        "    device_map['language_model.output'] = 0\n",
        "    device_map['language_model.model.norm'] = 0\n",
        "    device_map['language_model.lm_head'] = 0\n",
        "    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
        "\n",
        "    return device_map\n",
        "\n",
        "# If you have an 80G A100 GPU, you can put the entire model on a single GPU.\n",
        "# Otherwise, you need to load a model using multiple GPUs, please refer to the `Multiple GPUs` section.\n",
        "path = \"OpenGVLab/InternVL2-1B\"\n",
        "device_map = split_model('InternVL2-1B')\n",
        "model = AutoModel.from_pretrained(\n",
        "    path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    use_flash_attn=True,\n",
        "    trust_remote_code=True,\n",
        "    device_map=device_map).eval()\n",
        "\n",
        "# set the max number of tiles in `max_num`\n",
        "pixel_values = load_image('./img/high_res_image.png', max_num=12).to(torch.bfloat16).cuda()\n",
        "generation_config = dict(max_new_tokens=1024, do_sample=False)\n",
        "\n",
        "# pure-text conversation (纯文本对话)\n",
        "question = 'Hello, who are you?'\n",
        "response, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\n",
        "print(f'User: {question}\\nAssistant: {response}')\n",
        "\n",
        "question = 'Can you tell me a story?'\n",
        "response, history = model.chat(tokenizer, None, question, generation_config, history=history, return_history=True)\n",
        "print(f'User: {question}\\nAssistant: {response}')\n",
        "\n",
        "# single-image single-round conversation (单图单轮对话)\n",
        "question = '<image>\\nPlease describe the image shortly.'\n",
        "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
        "print(f'User: {question}\\nAssistant: {response}')\n",
        "\n",
        "# single-image multi-round conversation (单图多轮对话)\n",
        "question = '<image>\\nPlease describe the image in detail.'\n",
        "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
        "print(f'User: {question}\\nAssistant: {response}')\n",
        "\n",
        "question = 'Please write a poem according to the image.'\n",
        "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
        "print(f'User: {question}\\nAssistant: {response}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import math\n",
        "\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "def build_transform(input_size):\n",
        "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
        "    transform = T.Compose([\n",
        "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
        "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=MEAN, std=STD)\n",
        "    ])\n",
        "    return transform\n",
        "\n",
        "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
        "    best_ratio_diff = float('inf')\n",
        "    best_ratio = (1, 1)\n",
        "    area = width * height\n",
        "    for ratio in target_ratios:\n",
        "        target_aspect_ratio = ratio[0] / ratio[1]\n",
        "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
        "        if ratio_diff < best_ratio_diff:\n",
        "            best_ratio_diff = ratio_diff\n",
        "            best_ratio = ratio\n",
        "        elif ratio_diff == best_ratio_diff:\n",
        "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
        "                best_ratio = ratio\n",
        "    return best_ratio\n",
        "\n",
        "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
        "    orig_width, orig_height = image.size\n",
        "    aspect_ratio = orig_width / orig_height\n",
        "\n",
        "    # calculate the existing image aspect ratio\n",
        "    target_ratios = set(\n",
        "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
        "        i * j <= max_num and i * j >= min_num)\n",
        "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
        "\n",
        "    # find the closest aspect ratio to the target\n",
        "    target_aspect_ratio = find_closest_aspect_ratio(\n",
        "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
        "\n",
        "    # calculate the target width and height\n",
        "    target_width = image_size * target_aspect_ratio[0]\n",
        "    target_height = image_size * target_aspect_ratio[1]\n",
        "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
        "\n",
        "    # resize the image\n",
        "    resized_img = image.resize((target_width, target_height))\n",
        "    processed_images = []\n",
        "    for i in range(blocks):\n",
        "        box = (\n",
        "            (i % (target_width // image_size)) * image_size,\n",
        "            (i // (target_width // image_size)) * image_size,\n",
        "            ((i % (target_width // image_size)) + 1) * image_size,\n",
        "            ((i // (target_width // image_size)) + 1) * image_size\n",
        "        )\n",
        "        # split the image\n",
        "        split_img = resized_img.crop(box)\n",
        "        processed_images.append(split_img)\n",
        "    assert len(processed_images) == blocks\n",
        "    if use_thumbnail and len(processed_images) != 1:\n",
        "        thumbnail_img = image.resize((image_size, image_size))\n",
        "        processed_images.append(thumbnail_img)\n",
        "    return processed_images\n",
        "\n",
        "def load_image(image_file, input_size=448, max_num=12):\n",
        "    image = Image.open(image_file).convert('RGB')\n",
        "    transform = build_transform(input_size=input_size)\n",
        "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
        "    pixel_values = [transform(image) for image in images]\n",
        "    pixel_values = torch.stack(pixel_values)\n",
        "    return pixel_values\n",
        "\n",
        "def split_model(model_name):\n",
        "    device_map = {}\n",
        "    world_size = torch.cuda.device_count()\n",
        "\n",
        "    # Check if there are GPUs available\n",
        "    if world_size < 1:\n",
        "        raise RuntimeError(\"No GPUs are available. Please check your setup.\")\n",
        "\n",
        "    num_layers = {\n",
        "        'InternVL2-1B': 24, 'InternVL2-2B': 24, 'InternVL2-4B': 32, 'InternVL2-8B': 32,\n",
        "        'InternVL2-26B': 48, 'InternVL2-40B': 60, 'InternVL2-Llama3-76B': 80}[model_name]\n",
        "\n",
        "    # Calculate the number of layers per GPU\n",
        "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
        "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
        "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
        "\n",
        "    if len(num_layers_per_gpu) == 0:\n",
        "        raise ValueError(\"The `num_layers_per_gpu` list is empty. Check the world_size calculation.\")\n",
        "\n",
        "    layer_cnt = 0\n",
        "    for i, num_layer in enumerate(num_layers_per_gpu):\n",
        "        for j in range(num_layer):\n",
        "            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
        "            layer_cnt += 1\n",
        "    device_map['vision_model'] = 0\n",
        "    device_map['mlp1'] = 0\n",
        "    device_map['language_model.model.tok_embeddings'] = 0\n",
        "    device_map['language_model.model.embed_tokens'] = 0\n",
        "    device_map['language_model.output'] = 0\n",
        "    device_map['language_model.model.norm'] = 0\n",
        "    device_map['language_model.lm_head'] = 0\n",
        "    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
        "\n",
        "    return device_map\n",
        "\n",
        "# If you have an 80G A100 GPU, you can put the entire model on a single GPU.\n",
        "# Otherwise, you need to load a model using multiple GPUs, please refer to the `Multiple GPUs` section.\n",
        "path = \"OpenGVLab/InternVL2-1B\"\n",
        "device_map = split_model('InternVL2-1B')\n",
        "model = AutoModel.from_pretrained(\n",
        "    path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    use_flash_attn=True,\n",
        "    trust_remote_code=True,\n",
        "    device_map=device_map).eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
        "# set the max number of tiles in `max_num`\n",
        "pixel_values = load_image('./high_res_image.png', max_num=12).to(torch.bfloat16).cuda()\n",
        "generation_config = dict(max_new_tokens=1024, do_sample=False)\n",
        "\n",
        "# pure-text conversation (纯文本对话)\n",
        "question = 'Hello, who are you?'\n",
        "response, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\n",
        "print(f'User: {question}\\nAssistant: {response}')\n",
        "\n",
        "question = 'Can you tell me a story?'\n",
        "response, history = model.chat(tokenizer, None, question, generation_config, history=history, return_history=True)\n",
        "print(f'User: {question}\\nAssistant: {response}')\n",
        "\n",
        "# single-image single-round conversation (单图单轮对话)\n",
        "question = '<image>\\nPlease describe the image shortly.'\n",
        "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
        "print(f'User: {question}\\nAssistant: {response}')\n",
        "\n",
        "# single-image multi-round conversation (单图多轮对话)\n",
        "question = '<image>\\nPlease find the header and footer of this image'\n",
        "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
        "print(f'User: {question}\\nAssistant: {response}')\n",
        "\n",
        "question = 'Please write a poem according to the image.'\n",
        "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
        "print(f'User: {question}\\nAssistant: {response}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ETvw5CZh4a_",
        "outputId": "4173858d-1cc5-4f2f-cbba-6e7ca826d7b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Hello, who are you?\n",
            "Assistant: I am an AI assistant whose name is InternVL, developed jointly by Shanghai AI Lab and SenseTime.\n",
            "User: Can you tell me a story?\n",
            "Assistant: Sure, here's a story:\n",
            "\n",
            "Once upon a time, there was a young girl named Lily who lived in a small village. She was a curious and adventurous girl who loved exploring the world around her. One day, while wandering through the forest, she stumbled upon a hidden cave. Excited by the prospect of discovering something new, she decided to explore the cave.\n",
            "\n",
            "As she entered the cave, she was amazed by the beauty of the surroundings. The cave was filled with colorful flowers, sparkling crystals, and strange creatures. Lily was fascinated by the world she had discovered and decided to stay in the cave for a while.\n",
            "\n",
            "Over the next few days, Lily spent her days exploring the cave, learning about the different creatures and plants that lived there. She also discovered a hidden treasure that she had never seen before. It was a beautiful crystal that had been stolen from the cave by a group of thieves.\n",
            "\n",
            "When Lily returned to the village, she was hailed as a hero. The villagers were amazed by her bravery and thanked her for saving their village from the thieves. Lily realized that she had discovered something truly special and that she had made a difference in the world.\n",
            "\n",
            "From that day on, Lily became known as a brave and adventurous girl who had discovered something truly special. She continued to explore the world around her and make new friends, always eager to learn and grow.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: <image>\n",
            "Please describe the image shortly.\n",
            "Assistant: The image is a medical report from Mon Health Medical Center, detailing the patient's admission, medical history, and physical examination results. Here is a detailed description of the contents:\n",
            "\n",
            "### Patient Information:\n",
            "- **Patient Name:** LORING, TINA MARIE\n",
            "- **MRN:** 342017\n",
            "- **DOB/Gender:** 10/20/1969\n",
            "- **Account No.:** 62553143\n",
            "- **Location:** 5S; S505; 01\n",
            "- **Admission Date:** 5/2/2023\n",
            "- **Attending MD:** Mendapara, Vivek MD\n",
            "\n",
            "### Admission Documents:\n",
            "- **Date of Service:** 5/2/2023\n",
            "- **Chief Complaint:** sent by pcp for abscess on L buttock/rectal area x1.5 weeks that has grown in size.\n",
            "- **History of Present Illness:** A 53-year-old female with significant past medical history of diabetes mellitus, history of stroke without any residual deficit, hyperlipidemia who presented to the hospital with complaining of left buttock pain. She reported she has been having this pain over the last more than a week which was getting worse. She was having discomfort over her left buttock. Initially she thought it might be small swelling and will go away but is getting bigger. Never had similar problem in the past. Never had similar problem in the past. She reported multiple urinary tract infection and vaginal infection due to diabetes. She smokes 1 pack/day. Denies any fever but reported chills. Denies any vaginal discharge currently.\n",
            "- **Review of Systems:** Pertinent positives above in HPI\n",
            "- **Remaining 12 point ROS has been reviewed and is negative**\n",
            "\n",
            "### Physical Exam:\n",
            "- **Vitals & Measurements:** \n",
            "  - **HR:** 100\n",
            "  - **RR:** 18\n",
            "  - **BP:** 172/92\n",
            "  - **SpO2:** 100%\n",
            "  - **WT:** 71.7 kg\n",
            "  - **Gen:** NAD\n",
            "  - **CV:** RRR\n",
            "  - **Resp:** No rhonchi, nonlabored breathing\n",
            "  - **Skin:** warm\n",
            "  - **Abd:** nontender, nondistended, BSS+\n",
            "  - **Ext:** normal bulk and tone. Large, soft, firm, erythematous area noted in left perineal region extending towards rectum. No drainage. TTP, warm to touch.\n",
            "  - **Neuro:** CNS symmetrical\n",
            "  - **Psych:** appropriate affect, A&O to person, place, and situation\n",
            "\n",
            "### Medical Decision Making:\n",
            "- **Discussion of Advanced Care Planning:** Addressed ease of symptoms or control of pain\n",
            "- **Used care team approach to complete goals of care**\n",
            "\n",
            "### Summary:\n",
            "The report includes a detailed medical history, physical examination, and medical decision-making. The patient's vital signs, physical exam results, and discussion of advanced care planning are all documented. The report also includes a summary of the patient's medical history and the results of the physical examination.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: <image>\n",
            "Please find the header and footer of this image\n",
            "Assistant: Certainly! Here is the detailed content of the header and footer of the image:\n",
            "\n",
            "### Header:\n",
            "- **Mon Health Medical Center**\n",
            "- **1200 JD Anderson Drive**\n",
            "- **Morgantown, WV 26505**\n",
            "\n",
            "### Footer:\n",
            "- **Report Request ID: 41075457**\n",
            "- **Printed By: Eeragoni, Mahesh**\n",
            "- **Pg #: Page 9 of 210**\n",
            "\n",
            "### Header Details:\n",
            "- **Mon Health Medical Center**\n",
            "- **1200 JD Anderson Drive**\n",
            "- **Morgantown, WV 26505**\n",
            "\n",
            "### Footer Details:\n",
            "- **Report Request ID: 41075457**\n",
            "- **Printed By: Eeragoni, Mahesh**\n",
            "- **Pg #: Page 9 of 210**\n",
            "\n",
            "### Header and Footer Text:\n",
            "- **Mon Health Medical Center**\n",
            "- **1200 JD Anderson Drive**\n",
            "- **Morgantown, WV 26505**\n",
            "\n",
            "### Header Details:\n",
            "- **Patient Name:**\n",
            "  - **LORING, TINA MARIE**\n",
            "  - **MRN:**\n",
            "    - **342017**\n",
            "  - **DOB/Gender:**\n",
            "    - **10/20/1969**\n",
            "  - **Account No.:**\n",
            "    - **62553143**\n",
            "\n",
            "### Footer Details:\n",
            "- **Location:**\n",
            "  - **5S; S505; 01**\n",
            "- **Admission Date:**\n",
            "  - **5/2/2023**\n",
            "- **Attending MD:**\n",
            "  - **Mendapara, Vivek MD**\n",
            "\n",
            "### Header Details:\n",
            "- **Date of Service:**\n",
            "  - **5/2/2023**\n",
            "- **Chief Complaint:**\n",
            "  - **sent by pcp for abscess on L buttock/rectal area x1.5 weeks that has grown in size.**\n",
            "\n",
            "### Footer Details:\n",
            "- **Problem List/Past Medical History:**\n",
            "  - **Ongoing**\n",
            "  - **Diabetes mellitus**\n",
            "  - **Multiple sclerosis**\n",
            "  - **Stroke**\n",
            "  - **Vitamin D deficiency**\n",
            "  - **Historical**\n",
            "    - **No qualifying data**\n",
            "- **History:**\n",
            "  - **Alopecia - Low Risk, 04/20/2022**\n",
            "  - **Current - 1-2 times per week, 02/28/2023**\n",
            "  - **Substance Abuse - Denies Substance Abuse, 04/05/2021**\n",
            "  - **Never, 12/05/2022**\n",
            "  - **Tobacco - High Risk, 04/05/2021**\n",
            "  - **Smoking Tobacco Use: 10 or more**\n",
            "  - **Cigarettes (1/2 pack or more)/day in last 10 days. Smokeless Tobacco Use: Never, 02/28/2023**\n",
            "- **Family History:**\n",
            "  - **No family history is negative**\n",
            "- **Physical Exam:**\n",
            "  - **Vitals & Measurements:**\n",
            "    - **HR: 100**\n",
            "    - **RR: 18**\n",
            "    - **BP: 172/92**\n",
            "    - **SpO2: 100%**\n",
            "    - **WT: 71.7 kg**\n",
            "  - **Gen: NAD**\n",
            "  - **CV: RR**\n",
            "  - **Resp: No rhonchi, nonlabored breathing**\n",
            "  - **Skin: warm**\n",
            "  - **Abd: nontender, nondistended, BSS+**\n",
            "  - **Ext: normal bulk and tone. Large, soft, firm, erythematous area noted in left perineal region extending towards rectum. no drainage. TTP, warm to touch.**\n",
            "  - **Neuro: CNS symmetrical**\n",
            "  - **Psych: appropriate affect, A&O to person, place, and situation**\n",
            "- **Medical Decision Making:**\n",
            "  - **I have seen this patient personally and take history from patient/relative along with physical examination. Review vital signs, labs, imaging studies, previous records. Discussed diagnosis with the patient and prognosis. Patient agrees with the following plan.**\n",
            "  - **A=Abnormal C Level >60**\n",
            "  - **eGRF POC if >60**\n",
            "  - **Glucose Level 333 mg/dL High**\n",
            "  - **BUN 16 mg/dL Low**\n",
            "  - **Sodium Level 134 mmol/L Low**\n",
            "  - **Potassium Level 4.3 mmol\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Please write a poem according to the image.\n",
            "Assistant: In the shadows of the medical center's walls,\n",
            "Where patients lie, their stories untold,\n",
            "Loring, a name that echoes through the halls,\n",
            "A patient with a story untold, a tale to tell.\n",
            "\n",
            "Her name, TINA MARIE, a woman of many lives,\n",
            "A patient with a past, a story untold.\n",
            "Her medical history, a tale of pain and strife,\n",
            "A patient whose story is told, a tale to be told.\n",
            "\n",
            "Her past, a history of strokes, a disease that won't be denied,\n",
            "A disease that leaves scars, a disease that's never gone.\n",
            "Her pain, a symptom of a disease, a disease that's never gone,\n",
            "A disease that leaves scars, a disease that's never gone.\n",
            "\n",
            "Her family, a family of doctors, a family of nurses,\n",
            "A family of doctors, a family of nurses.\n",
            "Their love, their care, their support, a love that never fades,\n",
            "A love that never fades, a love that never fades.\n",
            "\n",
            "Her story, a tale of pain, a story of struggle,\n",
            "A story of a patient, a story of a patient.\n",
            "Her pain, a symptom of a disease, a disease that's never gone,\n",
            "A disease that leaves scars, a disease that's never gone.\n",
            "\n",
            "Her story, a tale of pain, a story of struggle,\n",
            "A story of a patient, a story of a patient.\n",
            "Her pain, a symptom of a disease, a disease that's never gone,\n",
            "A disease that leaves scars, a disease that's never gone.\n",
            "\n",
            "Her story, a tale of pain, a story of struggle,\n",
            "A story of a patient, a story of a patient.\n",
            "Her pain, a symptom of a disease, a disease that's never gone,\n",
            "A disease that leaves scars, a disease that's never gone.\n",
            "\n",
            "Her story, a tale of pain, a story of struggle,\n",
            "A story of a patient, a story of a patient.\n",
            "Her pain, a symptom of a disease, a disease that's never gone,\n",
            "A disease that leaves scars, a disease that's never gone.\n",
            "\n",
            "Her story, a tale of pain, a story of struggle,\n",
            "A story of a patient, a story of a patient.\n",
            "Her pain, a symptom of a disease, a disease that's never gone,\n",
            "A disease that leaves scars, a disease that's never gone.\n",
            "\n",
            "Her story, a tale of pain, a story of struggle,\n",
            "A story of a patient, a story of a patient.\n",
            "Her pain, a symptom of a disease, a disease that's never gone,\n",
            "A disease that leaves scars, a disease that's never gone.\n",
            "\n",
            "Her story, a tale of pain, a story of struggle,\n",
            "A story of a patient, a story of a patient.\n",
            "Her pain, a symptom of a disease, a disease that's never gone,\n",
            "A disease that leaves scars, a disease that's never gone.\n",
            "\n",
            "Her story, a tale of pain, a story of struggle,\n",
            "A story of a patient, a story of a patient.\n",
            "Her pain, a symptom of a disease, a disease that's never gone,\n",
            "A disease that leaves scars, a disease that's never gone.\n",
            "\n",
            "Her story, a tale of pain, a story of struggle,\n",
            "A story of a patient, a story of a patient.\n",
            "Her pain, a symptom of a disease, a disease that's never gone,\n",
            "A disease that leaves scars, a disease that's never gone.\n",
            "\n",
            "Her story, a tale of pain, a story of struggle,\n",
            "A story of a patient, a story of a patient.\n",
            "Her pain, a symptom of a disease, a disease that's never gone,\n",
            "A disease that leaves scars, a disease that's never gone.\n",
            "\n",
            "Her story, a tale of pain, a story of struggle,\n",
            "A story of a patient, a story of a patient.\n",
            "Her pain, a symptom of a disease, a disease that's never gone,\n",
            "A disease that leaves scars, a disease that's never gone.\n",
            "\n",
            "Her story, a tale of pain, a story of struggle,\n",
            "A story of a patient, a story of a patient.\n",
            "Her pain, a symptom of a disease, a disease that's never gone,\n",
            "A disease that leaves scars, a disease that's never gone.\n",
            "\n",
            "Her story, a tale of pain, a story of struggle,\n",
            "A story of a patient, a story of a patient.\n",
            "Her pain, a symptom of a disease, a disease that's never gone,\n",
            "A disease that leaves scars, a disease that's never gone.\n",
            "\n",
            "Her story, a tale of pain, a story of struggle,\n",
            "A story of a patient, a story of a patient.\n",
            "Her pain, a symptom of a disease, a disease that's never gone,\n",
            "A disease that leaves scars, a disease that's never gone.\n",
            "\n",
            "Her story, a tale of pain, a story of struggle,\n",
            "A story of a patient, a story of a patient.\n",
            "Her pain, a symptom of a disease, a disease that's never gone,\n",
            "A disease that leaves scars, a disease that's never gone.\n",
            "\n",
            "Her story, a tale of pain, a story of struggle,\n"
          ]
        }
      ]
    }
  ]
}